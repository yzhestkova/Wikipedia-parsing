{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code downloads Wikipedia dump files, parses them and extracts title and article text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code is a shorten and modified version of the project \"Wikipedia Data Science: Working with the Worldâ€™s Largest Encyclopedia\" by Will Koehrsen. Full code for his project with a very clear description can be retrieved from: https://github.com/WillKoehrsen/wikipedia-data-science/blob/master/notebooks/Downloading%20and%20Parsing%20Wikipedia%20Articles.ipynb "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###### Step 1: download raw data files from Wikimedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Parsing HTML\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# File system management\n",
    "import os\n",
    "\n",
    "# Libraries we will use as we go\n",
    "import pandas as pd\n",
    "import bz2\n",
    "import subprocess\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of the Wiki data\n",
    "base_url = 'https://dumps.wikimedia.org/enwiki/'\n",
    "index = requests.get(base_url).text\n",
    "soup_index = BeautifulSoup(index, 'html.parser')\n",
    "\n",
    "# Find the links that are dates of dumps\n",
    "dumps = [a['href'] for a in soup_index.find_all('a') if \n",
    "         a.has_attr('href')]\n",
    "\n",
    "dump_url = base_url + '20190920/' #Date stands for the version of the files\n",
    "\n",
    "# Retrieve the html\n",
    "dump_html = requests.get(dump_url).text\n",
    "\n",
    "# Convert to a soup\n",
    "soup_dump = BeautifulSoup(dump_html, 'html.parser')\n",
    "\n",
    "# Find elements with the class file\n",
    "soup_dump.find_all('li', {'class': 'file'}, limit = 10)[:4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would list all files, but we need only the most recent version of the articles\n",
    "files = []\n",
    "\n",
    "# Search through all files\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    # Select the relevant files\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "\n",
    "# We don't need meta data, just xml files of articles\n",
    "files_to_download = [file[0] for file in files if '.xml-p' in file[0]]\n",
    "\n",
    "# Make sure you don't accidentally download irrelevant xmls\n",
    "files_to_download=files_to_download[0:57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries that help download data in bulks\n",
    "import sys\n",
    "from keras.utils import get_file\n",
    "keras_home = '/Users/yulia_zhestkova/Wikidata/' #specify where to safe\n",
    "# You can also manually download what you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "# Iterate through each file\n",
    "for file in files_to_download:\n",
    "    path = keras_home + file\n",
    "    \n",
    "    # Check to see if the path exists (if the file is already downloaded)\n",
    "    if not os.path.exists(keras_home + file):\n",
    "        print('Downloading')\n",
    "        # If not, download the file\n",
    "        data_paths.append(get_file(file, dump_url))\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # If the file is already downloaded find some information\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        # Find the file size in MB\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "##### Step 2: Extracting relevant information using XML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    #Content handler for Wiki XML data\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        #Characters between opening and closing tags\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        #Opening tag of element\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        #Closing tag of element\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._pages.append((self._values['title'], self._values['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    lines.append(line)\n",
    "    \n",
    "    #if i > 5e5:\n",
    "     #   break\n",
    "\n",
    "#Check the structure of XML, see if there are anything specific about this version of the files\n",
    "lines[-112:-32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import mwparserfromhell \n",
    "\n",
    "start=timer()\n",
    "# Loop through the downloaded files and pass the handler to a SAX parser\n",
    "for j in range(1,57):\n",
    "    print(\"Processing partition {}\".format(j)) \n",
    "    data_path=data_paths[j]\n",
    "    handler = WikiXmlHandler()\n",
    "\n",
    "    # Parsing object\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                    stdin = open(data_path), \n",
    "                    stdout = subprocess.PIPE).stdout):\n",
    "        parser.feed(line)\n",
    "\n",
    "    df=pd.DataFrame(list(handler._pages))\n",
    "    df.columns = ['title', 'text']\n",
    "    df['text'] = df.text.apply(mwparserfromhell.parse)\n",
    "    df['text']=df['text'].str[:15000] #saving only first n symbols from the text for memory management\n",
    "    df.is_copy = None\n",
    "    df=df[df['text'].str.len()>300] #getting rid of abnormally short articles\n",
    "    df['first']=df['text'].str[:9]\n",
    "    df = df[df['first'] != \"#REDIRECT\"] #getting rid of doubled articles due to redirect\n",
    "    df=df.drop(['first'], axis=1)\n",
    "    partition_dir = '/Users/yulia_zhestkova/Wikipedia parsing/'\n",
    "    export_csv = df.to_csv (r'output'+str(j)+'.csv', index = None)\n",
    "\n",
    "    # Memory management\n",
    "    del handler\n",
    "    del parser\n",
    "    del df\n",
    "\n",
    "end=timer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "####### outputN.csv files now have processed dataframes with title and text of the articles. Note that the text data still needs lots of cleaninig."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
